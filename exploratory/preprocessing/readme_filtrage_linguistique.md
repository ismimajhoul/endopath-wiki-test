## √âtape 3 ‚Äî Filtrage linguistique (SpaCy)

### Fichier

`filter_tokens_with_spacy.py`

### Objectif

√Ä partir d‚Äôun vocabulaire brut (issu de l‚Äôextraction dossier gyn√©co), **classifier les tokens** en :

* **valides** (√† conserver),
* **invalides** (bruit / artefacts / tokens non pertinents),
* **√† corriger** (candidats pour corrections / enrichissement dictionnaires).

Cette √©tape sert √† **r√©duire le bruit** et √† cr√©er un ‚Äúbacklog‚Äù propre pour l‚Äô√©tape de suggestions (√âtape E).

### Entr√©es (inputs)

Sources attendues dans ton arborescence :

* `Data/DATA_PROCESSED/vocab_dossier_gyneco_from_xlsx.csv`
  (produit par `extract_text_and_vocab_from_dossier_gyneco.py`)

Selon ta version, il peut aussi utiliser :

* `Data/DATA_PROCESSED/all_words.csv` (si tu passes par une √©tape de build tokens/vocab interm√©diaire)

### Sorties (outputs)

Dans `Data/DATA_PROCESSED/` :

* `tokens_valides.csv`
* `tokens_invalides.csv`
* `tokens_a_corriger.csv`

üëâ Ces 3 fichiers sont des **artefacts de pipeline**, g√©n√©ralement **non versionn√©s**.

### Modules Python utilis√©s (et pourquoi)

Typiquement (√† confirmer sur ton code exact si tu r√©-uploade le fichier) :

* `pandas` : lecture/√©criture CSV, tri, regroupements, colonnes de score/flags.
* `spacy` : analyse linguistique FR (lemmatisation, POS, stopwords, etc.).
* `wordfreq` (ZIPF) : rep√©rer les tokens tr√®s rares ‚Üí suspects.
* `re` : r√®gles de filtrage regex (ponctuation, tokens mixtes, patterns parasites).
* `pathlib.Path` : chemins robustes.
* √©ventuellement `unicodedata` : normalisation (accents / caract√®res sp√©ciaux).

### D√©roul√© interne (logique ‚Äúpipeline‚Äù)

1. **Chargement du vocab brut**
   Le script lit un CSV contenant au minimum un champ ‚Äútoken‚Äù (souvent `token_source` ou `token`) et potentiellement des infos d‚Äôoccurrence/fr√©quence.

2. **Normalisation**
   Exemples typiques :

   * trim, lowercase (selon strat√©gie),
   * suppression de tokens vides,
   * homog√©n√©isation apostrophes / tirets,
   * filtrage des tokens trop courts / trop longs.

3. **Analyse SpaCy**
   Passage des tokens dans `fr_core_news_md` (ou similaire) pour :

   * d√©tecter ponctuation pure,
   * stopwords,
   * tokens non alpha (ou alpha+mix),
   * POS / shape (utile pour heuristiques).

4. **Score de raret√© (ZIPF)**
   Avec `wordfreq.zipf_frequency(token, "fr")` :

   * tokens tr√®s rares ‚Üí plus de chances d‚Äô√™tre erreurs de saisie, OCR-like, concat√©nations, etc.
   * ces tokens sont orient√©s vers `tokens_a_corriger.csv`.

5. **Classification**
   R√®gles usuelles :

   * **valides** : alpha, longueur raisonnable, pas stopword, pas trop rare, pas pattern parasite.
   * **invalides** : ponctuation, suites de symboles, tokens de ‚Äúformatage‚Äù, etc.
   * **√† corriger** : tokens plausibles mais rares / suspects / variants sans accents / abr√©viations non reconnues.

6. **√âcriture des 3 CSV**
   Les fichiers servent ensuite √† :

   * alimenter les dictionnaires / suggestions,
   * √©ventuellement v√©rifier l‚Äôimpact apr√®s enrichissement dico (re-run pipeline).

### Comment ex√©cuter

Depuis `exploratory/preprocessing/` :

```bash
python filter_tokens_with_spacy.py
```

### Pr√©requis

* SpaCy FR :

```bash
python -m spacy download fr_core_news_md
```

### Contr√¥les rapides

* `tokens_a_corriger.csv` doit √™tre non vide (sauf si le corpus est d√©j√† tr√®s propre).
* V√©rifier quelques lignes : tokens ‚Äúbizarres‚Äù, sans accents, abr√©viations, etc.

---

