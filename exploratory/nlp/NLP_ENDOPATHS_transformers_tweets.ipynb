{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "660899d0-10c4-412a-a9bd-f84f144c0ae5",
   "metadata": {},
   "source": [
    "#  NLP_ENDOPATHS_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1218cb-be18-460a-bfb9-f4149a565647",
   "metadata": {},
   "source": [
    "Author: Nicolai Wolpert\n",
    "\n",
    "Email: nicolai.wolpert@capgemini.com\n",
    "\n",
    "Date: July 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f26c3a2-a640-485a-a52a-6f4d5a75f92d",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88087d50-bd6f-4d06-a0ed-fb00f6bdef4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "\n",
    "# Data manipulation and other stuff : \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "#pd.set_option('display.max_rows', 10)\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "#import eli5 # eli5 not working anymore for current stable version of sklearn\n",
    "from IPython.display import display\n",
    "\n",
    "from preprocess_NLP import from_X_split_get_Y_split\n",
    "from sklearn.utils import resample, shuffle\n",
    "\n",
    "# Utils for NLP : \n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Utils for encoding : \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OrdinalEncoder, StandardScaler, OneHotEncoder\n",
    "\n",
    "# Utils for regression : \n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Utils for Multilabel classification :\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Utils for Metrics calculation : \n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, precision_recall_curve, hamming_loss, accuracy_score, jaccard_score, classification_report, roc_auc_score\n",
    "from sklearn.metrics import classification_report, roc_auc_score, accuracy_score, roc_curve\n",
    "from metrics_utils import *\n",
    "\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "# Multiclass/Multilabel preparation :\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# Kfold cross-validation with stratification\n",
    "from Opti_utils.ML_utils import kfold_cv_stratified\n",
    "\n",
    "# Custom preprocessing : \n",
    "from preprocess_NLP import preprocess_and_split\n",
    "\n",
    "# Tensorflow/keras\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.initializers import Constant\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dropout, Dense\n",
    "\n",
    "from pprint import pprint\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import CSVLogger\n",
    "from transformers import AutoModelForSequenceClassification, CamembertForMaskedLM, AutoTokenizer, AutoConfig\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "\n",
    "import plotly.express as px\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from datasets.dataset_dict import DatasetDict\n",
    "from datasets import Dataset\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ea8c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../Data/Tweets/'\n",
    "model_dir = '../../models/'\n",
    "pred_dir = '../../predictions/'\n",
    "\n",
    "# Choose pretrained model to use (e.g. 'camembert-base' or 'camembert/camembert-large')\n",
    "pretrained_model = 'camembert-base'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df27b5b",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c1c0862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>- Awww, c'est un bummer. Tu devrais avoir davi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Est contrarié qu'il ne puisse pas mettre à jou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>J'ai plongé plusieurs fois pour la balle. A ré...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Tout mon corps a des démangeaisons et comme si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Non, il ne se comporte pas du tout. je suis en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526719</th>\n",
       "      <td>1</td>\n",
       "      <td>Oui, cela fonctionne mieux que de l'attendre à...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526720</th>\n",
       "      <td>1</td>\n",
       "      <td>Je viens de me réveiller. Ne pas avoir d'école...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526721</th>\n",
       "      <td>1</td>\n",
       "      <td>Thewdb.com - très cool d'entendre les vieilles...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526722</th>\n",
       "      <td>1</td>\n",
       "      <td>Êtes-vous prêt pour votre mojo makeover? Deman...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526723</th>\n",
       "      <td>1</td>\n",
       "      <td>Joyeux 38ème anniversaire à mon livre de tous ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1526724 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                               text\n",
       "0            0  - Awww, c'est un bummer. Tu devrais avoir davi...\n",
       "1            0  Est contrarié qu'il ne puisse pas mettre à jou...\n",
       "2            0  J'ai plongé plusieurs fois pour la balle. A ré...\n",
       "3            0  Tout mon corps a des démangeaisons et comme si...\n",
       "4            0  Non, il ne se comporte pas du tout. je suis en...\n",
       "...        ...                                                ...\n",
       "1526719      1  Oui, cela fonctionne mieux que de l'attendre à...\n",
       "1526720      1  Je viens de me réveiller. Ne pas avoir d'école...\n",
       "1526721      1  Thewdb.com - très cool d'entendre les vieilles...\n",
       "1526722      1  Êtes-vous prêt pour votre mojo makeover? Deman...\n",
       "1526723      1  Joyeux 38ème anniversaire à mon livre de tous ...\n",
       "\n",
       "[1526724 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(data_dir + 'french_tweets.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc4924b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>- Awww, c'est un bummer. Tu devrais avoir davi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Est contrarié qu'il ne puisse pas mettre à jou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>J'ai plongé plusieurs fois pour la balle. A ré...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Tout mon corps a des démangeaisons et comme si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Non, il ne se comporte pas du tout. je suis en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772099</th>\n",
       "      <td>1</td>\n",
       "      <td>Commodité De la réparation &amp; lt; --- c'est jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772100</th>\n",
       "      <td>1</td>\n",
       "      <td>Karaoké avec xt sur les cours de mien et de ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772101</th>\n",
       "      <td>1</td>\n",
       "      <td>Tapez-le sur la publication du blog et imprime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772102</th>\n",
       "      <td>1</td>\n",
       "      <td>obi et Hornbach. (Via twitter.com): Obi et Hor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772103</th>\n",
       "      <td>1</td>\n",
       "      <td>Aw, merci ... espérons que oui! Je suis plus p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                               text\n",
       "0           0  - Awww, c'est un bummer. Tu devrais avoir davi...\n",
       "1           0  Est contrarié qu'il ne puisse pas mettre à jou...\n",
       "2           0  J'ai plongé plusieurs fois pour la balle. A ré...\n",
       "3           0  Tout mon corps a des démangeaisons et comme si...\n",
       "4           0  Non, il ne se comporte pas du tout. je suis en...\n",
       "...       ...                                                ...\n",
       "772099      1  Commodité De la réparation & lt; --- c'est jus...\n",
       "772100      1  Karaoké avec xt sur les cours de mien et de ba...\n",
       "772101      1  Tapez-le sur la publication du blog et imprime...\n",
       "772102      1  obi et Hornbach. (Via twitter.com): Obi et Hor...\n",
       "772103      1  Aw, merci ... espérons que oui! Je suis plus p...\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce number of data, else takes too long also choose class proportions for simulation\n",
    "\n",
    "nsamples_to_keep = 1000\n",
    "prop_neg = 0.5\n",
    "prop_pos = 1 - prop_neg\n",
    "nsamples_neg = int(nsamples_to_keep * prop_neg)\n",
    "nsamples_pos = int(nsamples_to_keep * prop_pos)\n",
    "\n",
    "data_neg = data.loc[data.label==0].head(nsamples_neg)\n",
    "data_pos = data.loc[data.label==1].head(nsamples_pos)\n",
    "data = pd.concat([data_neg, data_pos])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e076d0",
   "metadata": {},
   "source": [
    "### Camembert model accepts sentences with a maximum length of 512/514... check lengths of our sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78d91c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nwords = data['text'].str.split().apply(len).value_counts()\n",
    "plt.figure()\n",
    "plt.hist(nwords, bins=100)\n",
    "plt.title('Distribution nombre de mots')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0964ee2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model, use_fast = False)\n",
    "tokenized_data = tokenizer(data['text'].tolist())\n",
    "tokenized_data\n",
    "ntokens = [len(tokenized_data['input_ids'][i]) for i in range(len(tokenized_data['input_ids']))]\n",
    "plt.figure()\n",
    "plt.hist(ntokens, bins=100)\n",
    "plt.axvline(x=512, color='red')\n",
    "plt.title('Distribution nombre de tokens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3967a1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format labels\n",
    "def label_outcome(row):\n",
    "    if row['label'] == 0:\n",
    "        return 'negative'\n",
    "    elif row['label'] == 1:\n",
    "        return 'positive'\n",
    "data['str_label'] = data.apply(label_outcome, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f69473bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>str_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>- Awww, c'est un bummer. Tu devrais avoir davi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Est contrarié qu'il ne puisse pas mettre à jou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>J'ai plongé plusieurs fois pour la balle. A ré...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Tout mon corps a des démangeaisons et comme si...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Non, il ne se comporte pas du tout. je suis en...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772099</th>\n",
       "      <td>1</td>\n",
       "      <td>Commodité De la réparation &amp; lt; --- c'est jus...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772100</th>\n",
       "      <td>1</td>\n",
       "      <td>Karaoké avec xt sur les cours de mien et de ba...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772101</th>\n",
       "      <td>1</td>\n",
       "      <td>Tapez-le sur la publication du blog et imprime...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772102</th>\n",
       "      <td>1</td>\n",
       "      <td>obi et Hornbach. (Via twitter.com): Obi et Hor...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772103</th>\n",
       "      <td>1</td>\n",
       "      <td>Aw, merci ... espérons que oui! Je suis plus p...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        label                                               text str_label\n",
       "0           0  - Awww, c'est un bummer. Tu devrais avoir davi...  negative\n",
       "1           0  Est contrarié qu'il ne puisse pas mettre à jou...  negative\n",
       "2           0  J'ai plongé plusieurs fois pour la balle. A ré...  negative\n",
       "3           0  Tout mon corps a des démangeaisons et comme si...  negative\n",
       "4           0  Non, il ne se comporte pas du tout. je suis en...  negative\n",
       "...       ...                                                ...       ...\n",
       "772099      1  Commodité De la réparation & lt; --- c'est jus...  positive\n",
       "772100      1  Karaoké avec xt sur les cours de mien et de ba...  positive\n",
       "772101      1  Tapez-le sur la publication du blog et imprime...  positive\n",
       "772102      1  obi et Hornbach. (Via twitter.com): Obi et Hor...  positive\n",
       "772103      1  Aw, merci ... espérons que oui! Je suis plus p...  positive\n",
       "\n",
       "[1000 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335378dc",
   "metadata": {},
   "source": [
    "## Data preparation for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec25da17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model, use_fast = False)\n",
    "\n",
    "def tokenize_batch(samples, tokenizer, max_length):\n",
    "    text = [sample[\"text\"] for sample in samples]\n",
    "    labels = torch.tensor([sample[\"label\"] for sample in samples])\n",
    "    str_labels = [sample[\"str_label\"] for sample in samples]\n",
    "    # The tokenizer handles\n",
    "    # - Tokenization (amazing right?)\n",
    "    # - Padding (adding empty tokens so that each example has the same length)\n",
    "    # - Truncation (cutting samples that are too long)\n",
    "    # - Special tokens (in CamemBERT, each sentence ends with a special token </s>)\n",
    "    # - Attention mask (a binary vector which tells the model which tokens to look at. For instance it will not compute anything if the token is a padding token)\n",
    "    #tokens = tokenizer(text, padding=\"longest\", return_tensors=\"pt\")\n",
    "    tokens = tokenizer(text, padding='max_length', truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "    \n",
    "    return {\"input_ids\": tokens.input_ids, \"attention_mask\": tokens.attention_mask, \"labels\": labels, \"str_labels\": str_labels, \"sentences\": text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e61c49",
   "metadata": {},
   "source": [
    "### Train/Val/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08e196eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples in train set: 787, = 78.7 %\n",
      "Number of samples in validation set: 113, = 11.3 %\n",
      "Number of samples in test set: 100, = 10.0 %\n"
     ]
    }
   ],
   "source": [
    "prop_train = 0.8\n",
    "prop_val = 0.1\n",
    "prop_test = 0.1\n",
    "\n",
    "train, test = train_test_split(data, random_state=42, test_size=prop_test, stratify=data['label'])\n",
    "train, val = train_test_split(train, random_state=42, test_size=0.125, stratify=train['label'])\n",
    "\n",
    "print(f'Number of samples in train set: {train.shape[0]}, = {round((train.shape[0]/data.shape[0])*100, 2)} %')\n",
    "print(f'Number of samples in validation set: {val.shape[0]}, = {round((val.shape[0]/data.shape[0])*100, 2)} %')\n",
    "print(f'Number of samples in test set: {test.shape[0]}, = {round((test.shape[0]/data.shape[0])*100, 2)} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1e795322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vérifier bonne distribution des classes dans train/val/test\n",
    "fig, axs = plt.subplots(nrows=1, ncols=3)\n",
    "axs = axs.flatten()\n",
    "sns.histplot(data=train['str_label'], shrink=0.3, color='#00B2A2', ax=axs[0])\n",
    "axs[0].bar_label(axs[0].containers[0])\n",
    "axs[0].set_xlabel('')\n",
    "axs[0].set_ylabel('Nombre', fontsize=14)\n",
    "axs[0].set_title('train', pad=25, fontsize=14)\n",
    "sns.histplot(data=val['str_label'], shrink=0.3, color='#00B2A2', ax=axs[1])\n",
    "axs[1].bar_label(axs[1].containers[0])\n",
    "axs[1].set_xlabel('')\n",
    "axs[1].set_ylabel('Nombre', fontsize=14)\n",
    "axs[1].set_ylim(axs[0].get_ylim())\n",
    "axs[1].set_title('val', pad=25, fontsize=14)\n",
    "sns.histplot(data=test['str_label'], shrink=0.3, color='#00B2A2', ax=axs[2])\n",
    "axs[2].bar_label(axs[2].containers[0])\n",
    "axs[2].set_xlabel('')\n",
    "axs[2].set_ylabel('Nombre', fontsize=14)\n",
    "axs[2].set_ylim(axs[0].get_ylim())\n",
    "axs[2].set_title('test', pad=25, fontsize=14)\n",
    "plt.suptitle('Distribution classes')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "23a192f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer les données en format qui peut être lu dans les librairies 'HuggingFace'\n",
    "\n",
    "max_length = 512\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "val_dataset = Dataset.from_pandas(val)\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset[\"train\"], \n",
    "    batch_size=16, \n",
    "    shuffle=True, \n",
    "    collate_fn=functools.partial(tokenize_batch, tokenizer=tokenizer, max_length=max_length)\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    dataset[\"validation\"], \n",
    "    batch_size=16, \n",
    "    shuffle=False, \n",
    "    collate_fn=functools.partial(tokenize_batch, tokenizer=tokenizer, max_length=max_length)\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    dataset[\"validation\"], \n",
    "    batch_size=16, \n",
    "    shuffle=False, \n",
    "    collate_fn=functools.partial(tokenize_batch, tokenizer=tokenizer, max_length=max_length)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f34d4a",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1641bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(self, model_name, num_labels, lr, weight_decay, from_scratch=False):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        if from_scratch:\n",
    "            # Si `from_scratch` est vrai, on charge uniquement la config (nombre de couches, hidden size, etc.) et pas les poids du modèle \n",
    "            config = AutoConfig.from_pretrained(\n",
    "                model_name, num_labels=num_labels\n",
    "            )\n",
    "            self.model = AutoModelForSequenceClassification.from_config(config)\n",
    "        else:\n",
    "            # Cette méthode permet de télécharger le bon modèle pré-entraîné directement depuis le Hub de HuggingFace sur lequel sont stockés de nombreux modèles\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_name, num_labels=num_labels\n",
    "            )\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.num_labels = self.model.num_labels\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"]\n",
    "        )\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        out = self.forward(batch)\n",
    "\n",
    "        logits = out.logits\n",
    "        # -------- MASKED --------\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        loss = loss_fn(logits.view(-1, self.num_labels), batch[\"labels\"].view(-1))\n",
    "\n",
    "        # ------ END MASKED ------\n",
    "\n",
    "        self.log(\"train/loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_index):\n",
    "        labels = batch[\"labels\"]\n",
    "        out = self.forward(batch)\n",
    "\n",
    "        preds = torch.max(out.logits, -1).indices\n",
    "        # -------- MASKED --------\n",
    "        acc = (batch[\"labels\"] == preds).float().mean()\n",
    "        # ------ END MASKED ------\n",
    "        self.log(\"valid/acc\", acc)\n",
    "\n",
    "        f1 = f1_score(batch[\"labels\"].cpu().tolist(), preds.cpu().tolist(), average=\"macro\")\n",
    "        self.log(\"valid/f1\", f1)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        \"\"\"La fonction predict step facilite la prédiction de données. Elle est \n",
    "        similaire à `validation_step`, sans le calcul des métriques.\n",
    "        \"\"\"\n",
    "        out = self.forward(batch)\n",
    "\n",
    "        return torch.max(out.logits, -1).indices\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(\n",
    "            self.model.parameters(), lr=self.lr, weight_decay=self.weight_decay\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fa6dd3d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = pl.callbacks.ModelCheckpoint(monitor=\"valid/acc\", mode=\"max\")\n",
    "\n",
    "# Set up CSV logger\n",
    "csv_logger = CSVLogger(\"logs\", name=\"camembert_training\")\n",
    "camembert_trainer = pl.Trainer(\n",
    "    max_epochs=20,\n",
    "    #gpus=1,\n",
    "    logger=csv_logger,\n",
    "    callbacks=[\n",
    "        pl.callbacks.EarlyStopping(monitor=\"valid/acc\", patience=4, mode=\"max\"),\n",
    "        model_checkpoint,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "41f1f29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 3e-5\n",
    "lightning_model = LightningModel(pretrained_model, num_labels=2, lr=learning_rate, weight_decay=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8a14a2ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type                               | Params | Mode\n",
      "--------------------------------------------------------------------\n",
      "0 | model | CamembertForSequenceClassification | 110 M  | eval\n",
      "--------------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "442.494   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1673285adc14492298d0d0eaafda4368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nwolpert\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\endopaths-Iae5WtKA-py3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\nwolpert\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\endopaths-Iae5WtKA-py3.10\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 16. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Users\\nwolpert\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\endopaths-Iae5WtKA-py3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f5034a4cf94e63831053e5220a518a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19477bb31c8d4e80bd164b24b6a3680f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nwolpert\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\endopaths-Iae5WtKA-py3.10\\lib\\site-packages\\pytorch_lightning\\utilities\\data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 1. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "c:\\Users\\nwolpert\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\endopaths-Iae5WtKA-py3.10\\lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "camembert_trainer.fit(lightning_model, train_dataloaders=train_dataloader, val_dataloaders=val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89b7701",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(camembert_trainer, open(model_dir + f'{pretrained_model}_sliding_window_learning_rate_{learning_rate}_tweets', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3261f4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m camembert_trainer \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mpretrained_model\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_sliding_window_learning_rate_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_tweets\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "camembert_trainer = pickle.load(open(model_dir + f'{pretrained_model}_sliding_window_learning_rate_{learning_rate}_tweets', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6083a16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'roberta.embeddings.word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LightningModel:\n\tMissing key(s) in state_dict: \"model.roberta.encoder.layer.12.attention.self.query.weight\", \"model.roberta.encoder.layer.12.attention.self.query.bias\", \"model.roberta.encoder.layer.12.attention.self.key.weight\", \"model.roberta.encoder.layer.12.attention.self.key.bias\", \"model.roberta.encoder.layer.12.attention.self.value.weight\", \"model.roberta.encoder.layer.12.attention.self.value.bias\", \"model.roberta.encoder.layer.12.attention.output.dense.weight\", \"model.roberta.encoder.layer.12.attention.output.dense.bias\", \"model.roberta.encoder.layer.12.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.12.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.12.intermediate.dense.weight\", \"model.roberta.encoder.layer.12.intermediate.dense.bias\", \"model.roberta.encoder.layer.12.output.dense.weight\", \"model.roberta.encoder.layer.12.output.dense.bias\", \"model.roberta.encoder.layer.12.output.LayerNorm.weight\", \"model.roberta.encoder.layer.12.output.LayerNorm.bias\", \"model.roberta.encoder.layer.13.attention.self.query.weight\", \"model.roberta.encoder.layer.13.attention.self.query.bias\", \"model.roberta.encoder.layer.13.attention.self.key.weight\", \"model.roberta.encoder.layer.13.attention.self.key.bias\", \"model.roberta.encoder.layer.13.attention.self.value.weight\", \"model.roberta.encoder.layer.13.attention.self.value.bias\", \"model.roberta.encoder.layer.13.attention.output.dense.weight\", \"model.roberta.encoder.layer.13.attention.output.dense.bias\", \"model.roberta.encoder.layer.13.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.13.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.13.intermediate.dense.weight\", \"model.roberta.encoder.layer.13.intermediate.dense.bias\", \"model.roberta.encoder.layer.13.output.dense.weight\", \"model.roberta.encoder.layer.13.output.dense.bias\", \"model.roberta.encoder.layer.13.output.LayerNorm.weight\", \"model.roberta.encoder.layer.13.output.LayerNorm.bias\", \"model.roberta.encoder.layer.14.attention.self.query.weight\", \"model.roberta.encoder.layer.14.attention.self.query.bias\", \"model.roberta.encoder.layer.14.attention.self.key.weight\", \"model.roberta.encoder.layer.14.attention.self.key.bias\", \"model.roberta.encoder.layer.14.attention.self.value.weight\", \"model.roberta.encoder.layer.14.attention.self.value.bias\", \"model.roberta.encoder.layer.14.attention.output.dense.weight\", \"model.roberta.encoder.layer.14.attention.output.dense.bias\", \"model.roberta.encoder.layer.14.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.14.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.14.intermediate.dense.weight\", \"model.roberta.encoder.layer.14.intermediate.dense.bias\", \"model.roberta.encoder.layer.14.output.dense.weight\", \"model.roberta.encoder.layer.14.output.dense.bias\", \"model.roberta.encoder.layer.14.output.LayerNorm.weight\", \"model.roberta.encoder.layer.14.output.LayerNorm.bias\", \"model.roberta.encoder.layer.15.attention.self.query.weight\", \"model.roberta.encoder.layer.15.attention.self.query.bias\", \"model.roberta.encoder.layer.15.attention.self.key.weight\", \"model.roberta.encoder.layer.15.attention.self.key.bias\", \"model.roberta.encoder.layer.15.attention.self.value.weight\", \"model.roberta.encoder.layer.15.attention.self.value.bias\", \"model.roberta.encoder.layer.15.attention.output.dense.weight\", \"model.roberta.encoder.layer.15.attention.output.dense.bias\", \"model.roberta.encoder.layer.15.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.15.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.15.intermediate.dense.weight\", \"model.roberta.encoder.layer.15.intermediate.dense.bias\", \"model.roberta.encoder.layer.15.output.dense.weight\", \"model.roberta.encoder.layer.15.output.dense.bias\", \"model.roberta.encoder.layer.15.output.LayerNorm.weight\", \"model.roberta.encoder.layer.15.output.LayerNorm.bias\", \"model.roberta.encoder.layer.16.attention.self.query.weight\", \"model.roberta.encoder.layer.16.attention.self.query.bias\", \"model.roberta.encoder.layer.16.attention.self.key.weight\", \"model.roberta.encoder.layer.16.attention.self.key.bias\", \"model.roberta.encoder.layer.16.attention.self.value.weight\", \"model.roberta.encoder.layer.16.attention.self.value.bias\", \"model.roberta.encoder.layer.16.attention.output.dense.weight\", \"model.roberta.encoder.layer.16.attention.output.dense.bias\", \"model.roberta.encoder.layer.16.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.16.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.16.intermediate.dense.weight\", \"model.roberta.encoder.layer.16.intermediate.dense.bias\", \"model.roberta.encoder.layer.16.output.dense.weight\", \"model.roberta.encoder.layer.16.output.dense.bias\", \"model.roberta.encoder.layer.16.output.LayerNorm.weight\", \"model.roberta.encoder.layer.16.output.LayerNorm.bias\", \"model.roberta.encoder.layer.17.attention.self.query.weight\", \"model.roberta.encoder.layer.17.attention.self.query.bias\", \"model.roberta.encoder.layer.17.attention.self.key.weight\", \"model.roberta.encoder.layer.17.attention.self.key.bias\", \"model.roberta.encoder.layer.17.attention.self.value.weight\", \"model.roberta.encoder.layer.17.attention.self.value.bias\", \"model.roberta.encoder.layer.17.attention.output.dense.weight\", \"model.roberta.encoder.layer.17.attention.output.dense.bias\", \"model.roberta.encoder.layer.17.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.17.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.17.intermediate.dense.weight\", \"model.roberta.encoder.layer.17.intermediate.dense.bias\", \"model.roberta.encoder.layer.17.output.dense.weight\", \"model.roberta.encoder.layer.17.output.dense.bias\", \"model.roberta.encoder.layer.17.output.LayerNorm.weight\", \"model.roberta.encoder.layer.17.output.LayerNorm.bias\", \"model.roberta.encoder.layer.18.attention.self.query.weight\", \"model.roberta.encoder.layer.18.attention.self.query.bias\", \"model.roberta.encoder.layer.18.attention.self.key.weight\", \"model.roberta.encoder.layer.18.attention.self.key.bias\", \"model.roberta.encoder.layer.18.attention.self.value.weight\", \"model.roberta.encoder.layer.18.attention.self.value.bias\", \"model.roberta.encoder.layer.18.attention.output.dense.weight\", \"model.roberta.encoder.layer.18.attention.output.dense.bias\", \"model.roberta.encoder.layer.18.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.18.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.18.intermediate.dense.weight\", \"model.roberta.encoder.layer.18.intermediate.dense.bias\", \"model.roberta.encoder.layer.18.output.dense.weight\", \"model.roberta.encoder.layer.18.output.dense.bias\", \"model.roberta.encoder.layer.18.output.LayerNorm.weight\", \"model.roberta.encoder.layer.18.output.LayerNorm.bias\", \"model.roberta.encoder.layer.19.attention.self.query.weight\", \"model.roberta.encoder.layer.19.attention.self.query.bias\", \"model.roberta.encoder.layer.19.attention.self.key.weight\", \"model.roberta.encoder.layer.19.attention.self.key.bias\", \"model.roberta.encoder.layer.19.attention.self.value.weight\", \"model.roberta.encoder.layer.19.attention.self.value.bias\", \"model.roberta.encoder.layer.19.attention.output.dense.weight\", \"model.roberta.encoder.layer.19.attention.output.dense.bias\", \"model.roberta.encoder.layer.19.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.19.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.19.intermediate.dense.weight\", \"model.roberta.encoder.layer.19.intermediate.dense.bias\", \"model.roberta.encoder.layer.19.output.dense.weight\", \"model.roberta.encoder.layer.19.output.dense.bias\", \"model.roberta.encoder.layer.19.output.LayerNorm.weight\", \"model.roberta.encoder.layer.19.output.LayerNorm.bias\", \"model.roberta.encoder.layer.20.attention.self.query.weight\", \"model.roberta.encoder.layer.20.attention.self.query.bias\", \"model.roberta.encoder.layer.20.attention.self.key.weight\", \"model.roberta.encoder.layer.20.attention.self.key.bias\", \"model.roberta.encoder.layer.20.attention.self.value.weight\", \"model.roberta.encoder.layer.20.attention.self.value.bias\", \"model.roberta.encoder.layer.20.attention.output.dense.weight\", \"model.roberta.encoder.layer.20.attention.output.dense.bias\", \"model.roberta.encoder.layer.20.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.20.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.20.intermediate.dense.weight\", \"model.roberta.encoder.layer.20.intermediate.dense.bias\", \"model.roberta.encoder.layer.20.output.dense.weight\", \"model.roberta.encoder.layer.20.output.dense.bias\", \"model.roberta.encoder.layer.20.output.LayerNorm.weight\", \"model.roberta.encoder.layer.20.output.LayerNorm.bias\", \"model.roberta.encoder.layer.21.attention.self.query.weight\", \"model.roberta.encoder.layer.21.attention.self.query.bias\", \"model.roberta.encoder.layer.21.attention.self.key.weight\", \"model.roberta.encoder.layer.21.attention.self.key.bias\", \"model.roberta.encoder.layer.21.attention.self.value.weight\", \"model.roberta.encoder.layer.21.attention.self.value.bias\", \"model.roberta.encoder.layer.21.attention.output.dense.weight\", \"model.roberta.encoder.layer.21.attention.output.dense.bias\", \"model.roberta.encoder.layer.21.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.21.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.21.intermediate.dense.weight\", \"model.roberta.encoder.layer.21.intermediate.dense.bias\", \"model.roberta.encoder.layer.21.output.dense.weight\", \"model.roberta.encoder.layer.21.output.dense.bias\", \"model.roberta.encoder.layer.21.output.LayerNorm.weight\", \"model.roberta.encoder.layer.21.output.LayerNorm.bias\", \"model.roberta.encoder.layer.22.attention.self.query.weight\", \"model.roberta.encoder.layer.22.attention.self.query.bias\", \"model.roberta.encoder.layer.22.attention.self.key.weight\", \"model.roberta.encoder.layer.22.attention.self.key.bias\", \"model.roberta.encoder.layer.22.attention.self.value.weight\", \"model.roberta.encoder.layer.22.attention.self.value.bias\", \"model.roberta.encoder.layer.22.attention.output.dense.weight\", \"model.roberta.encoder.layer.22.attention.output.dense.bias\", \"model.roberta.encoder.layer.22.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.22.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.22.intermediate.dense.weight\", \"model.roberta.encoder.layer.22.intermediate.dense.bias\", \"model.roberta.encoder.layer.22.output.dense.weight\", \"model.roberta.encoder.layer.22.output.dense.bias\", \"model.roberta.encoder.layer.22.output.LayerNorm.weight\", \"model.roberta.encoder.layer.22.output.LayerNorm.bias\", \"model.roberta.encoder.layer.23.attention.self.query.weight\", \"model.roberta.encoder.layer.23.attention.self.query.bias\", \"model.roberta.encoder.layer.23.attention.self.key.weight\", \"model.roberta.encoder.layer.23.attention.self.key.bias\", \"model.roberta.encoder.layer.23.attention.self.value.weight\", \"model.roberta.encoder.layer.23.attention.self.value.bias\", \"model.roberta.encoder.layer.23.attention.output.dense.weight\", \"model.roberta.encoder.layer.23.attention.output.dense.bias\", \"model.roberta.encoder.layer.23.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.23.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.23.intermediate.dense.weight\", \"model.roberta.encoder.layer.23.intermediate.dense.bias\", \"model.roberta.encoder.layer.23.output.dense.weight\", \"model.roberta.encoder.layer.23.output.dense.bias\", \"model.roberta.encoder.layer.23.output.LayerNorm.weight\", \"model.roberta.encoder.layer.23.output.LayerNorm.bias\". \n\tsize mismatch for model.roberta.embeddings.word_embeddings.weight: copying a param with shape torch.Size([32005, 768]) from checkpoint, the shape in current model is torch.Size([32005, 1024]).\n\tsize mismatch for model.roberta.embeddings.position_embeddings.weight: copying a param with shape torch.Size([514, 768]) from checkpoint, the shape in current model is torch.Size([514, 1024]).\n\tsize mismatch for model.roberta.embeddings.token_type_embeddings.weight: copying a param with shape torch.Size([1, 768]) from checkpoint, the shape in current model is torch.Size([1, 1024]).\n\tsize mismatch for model.roberta.embeddings.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.embeddings.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.0.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.0.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.1.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.1.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.2.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.2.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.3.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.3.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.4.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.4.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.5.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.5.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.6.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.6.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.7.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.7.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.8.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.8.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.9.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.9.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.10.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.10.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.11.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.11.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.classifier.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.classifier.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.classifier.out_proj.weight: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([2, 1024]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the best model checkpoint\u001b[39;00m\n\u001b[0;32m      5\u001b[0m best_model_path \u001b[38;5;241m=\u001b[39m model_checkpoint\u001b[38;5;241m.\u001b[39mbest_model_path\n\u001b[1;32m----> 6\u001b[0m best_model \u001b[38;5;241m=\u001b[39m \u001b[43mLightningModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcamembert/camembert-large\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3e-5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Predict on the test dataset\u001b[39;00m\n\u001b[0;32m      9\u001b[0m predictions \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\nwolpert\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\endopaths-Iae5WtKA-py3.10\\lib\\site-packages\\pytorch_lightning\\utilities\\model_helpers.py:125\u001b[0m, in \u001b[0;36m_restricted_classmethod_impl.__get__.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scripting:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe classmethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` cannot be called on an instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please call it on the class type and make sure the return value is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m     )\n\u001b[1;32m--> 125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmethod(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nwolpert\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\endopaths-Iae5WtKA-py3.10\\lib\\site-packages\\pytorch_lightning\\core\\module.py:1586\u001b[0m, in \u001b[0;36mLightningModule.load_from_checkpoint\u001b[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;129m@_restricted_classmethod\u001b[39m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_checkpoint\u001b[39m(\n\u001b[0;32m   1499\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1504\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1505\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m   1506\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Primary way of loading a model from a checkpoint. When Lightning saves a checkpoint it stores the arguments\u001b[39;00m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;124;03m    passed to ``__init__``  in the checkpoint under ``\"hyper_parameters\"``.\u001b[39;00m\n\u001b[0;32m   1508\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1584\u001b[0m \n\u001b[0;32m   1585\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1586\u001b[0m     loaded \u001b[38;5;241m=\u001b[39m _load_from_checkpoint(\n\u001b[0;32m   1587\u001b[0m         \u001b[38;5;28mcls\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1588\u001b[0m         checkpoint_path,\n\u001b[0;32m   1589\u001b[0m         map_location,\n\u001b[0;32m   1590\u001b[0m         hparams_file,\n\u001b[0;32m   1591\u001b[0m         strict,\n\u001b[0;32m   1592\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1593\u001b[0m     )\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(Self, loaded)\n",
      "File \u001b[1;32mc:\\Users\\nwolpert\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\endopaths-Iae5WtKA-py3.10\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:91\u001b[0m, in \u001b[0;36m_load_from_checkpoint\u001b[1;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pl\u001b[38;5;241m.\u001b[39mLightningModule):\n\u001b[1;32m---> 91\u001b[0m     model \u001b[38;5;241m=\u001b[39m _load_state(\u001b[38;5;28mcls\u001b[39m, checkpoint, strict\u001b[38;5;241m=\u001b[39mstrict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     92\u001b[0m     state_dict \u001b[38;5;241m=\u001b[39m checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m state_dict:\n",
      "File \u001b[1;32mc:\\Users\\nwolpert\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\endopaths-Iae5WtKA-py3.10\\lib\\site-packages\\pytorch_lightning\\core\\saving.py:187\u001b[0m, in \u001b[0;36m_load_state\u001b[1;34m(cls, checkpoint, strict, **cls_kwargs_new)\u001b[0m\n\u001b[0;32m    184\u001b[0m     obj\u001b[38;5;241m.\u001b[39mon_load_checkpoint(checkpoint)\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# load the state_dict on the model automatically\u001b[39;00m\n\u001b[1;32m--> 187\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstate_dict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m strict:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m keys\u001b[38;5;241m.\u001b[39mmissing_keys:\n",
      "File \u001b[1;32mc:\\Users\\nwolpert\\AppData\\Local\\pypoetry\\Cache\\virtualenvs\\endopaths-Iae5WtKA-py3.10\\lib\\site-packages\\torch\\nn\\modules\\module.py:2189\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2184\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2185\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2186\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2190\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LightningModel:\n\tMissing key(s) in state_dict: \"model.roberta.encoder.layer.12.attention.self.query.weight\", \"model.roberta.encoder.layer.12.attention.self.query.bias\", \"model.roberta.encoder.layer.12.attention.self.key.weight\", \"model.roberta.encoder.layer.12.attention.self.key.bias\", \"model.roberta.encoder.layer.12.attention.self.value.weight\", \"model.roberta.encoder.layer.12.attention.self.value.bias\", \"model.roberta.encoder.layer.12.attention.output.dense.weight\", \"model.roberta.encoder.layer.12.attention.output.dense.bias\", \"model.roberta.encoder.layer.12.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.12.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.12.intermediate.dense.weight\", \"model.roberta.encoder.layer.12.intermediate.dense.bias\", \"model.roberta.encoder.layer.12.output.dense.weight\", \"model.roberta.encoder.layer.12.output.dense.bias\", \"model.roberta.encoder.layer.12.output.LayerNorm.weight\", \"model.roberta.encoder.layer.12.output.LayerNorm.bias\", \"model.roberta.encoder.layer.13.attention.self.query.weight\", \"model.roberta.encoder.layer.13.attention.self.query.bias\", \"model.roberta.encoder.layer.13.attention.self.key.weight\", \"model.roberta.encoder.layer.13.attention.self.key.bias\", \"model.roberta.encoder.layer.13.attention.self.value.weight\", \"model.roberta.encoder.layer.13.attention.self.value.bias\", \"model.roberta.encoder.layer.13.attention.output.dense.weight\", \"model.roberta.encoder.layer.13.attention.output.dense.bias\", \"model.roberta.encoder.layer.13.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.13.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.13.intermediate.dense.weight\", \"model.roberta.encoder.layer.13.intermediate.dense.bias\", \"model.roberta.encoder.layer.13.output.dense.weight\", \"model.roberta.encoder.layer.13.output.dense.bias\", \"model.roberta.encoder.layer.13.output.LayerNorm.weight\", \"model.roberta.encoder.layer.13.output.LayerNorm.bias\", \"model.roberta.encoder.layer.14.attention.self.query.weight\", \"model.roberta.encoder.layer.14.attention.self.query.bias\", \"model.roberta.encoder.layer.14.attention.self.key.weight\", \"model.roberta.encoder.layer.14.attention.self.key.bias\", \"model.roberta.encoder.layer.14.attention.self.value.weight\", \"model.roberta.encoder.layer.14.attention.self.value.bias\", \"model.roberta.encoder.layer.14.attention.output.dense.weight\", \"model.roberta.encoder.layer.14.attention.output.dense.bias\", \"model.roberta.encoder.layer.14.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.14.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.14.intermediate.dense.weight\", \"model.roberta.encoder.layer.14.intermediate.dense.bias\", \"model.roberta.encoder.layer.14.output.dense.weight\", \"model.roberta.encoder.layer.14.output.dense.bias\", \"model.roberta.encoder.layer.14.output.LayerNorm.weight\", \"model.roberta.encoder.layer.14.output.LayerNorm.bias\", \"model.roberta.encoder.layer.15.attention.self.query.weight\", \"model.roberta.encoder.layer.15.attention.self.query.bias\", \"model.roberta.encoder.layer.15.attention.self.key.weight\", \"model.roberta.encoder.layer.15.attention.self.key.bias\", \"model.roberta.encoder.layer.15.attention.self.value.weight\", \"model.roberta.encoder.layer.15.attention.self.value.bias\", \"model.roberta.encoder.layer.15.attention.output.dense.weight\", \"model.roberta.encoder.layer.15.attention.output.dense.bias\", \"model.roberta.encoder.layer.15.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.15.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.15.intermediate.dense.weight\", \"model.roberta.encoder.layer.15.intermediate.dense.bias\", \"model.roberta.encoder.layer.15.output.dense.weight\", \"model.roberta.encoder.layer.15.output.dense.bias\", \"model.roberta.encoder.layer.15.output.LayerNorm.weight\", \"model.roberta.encoder.layer.15.output.LayerNorm.bias\", \"model.roberta.encoder.layer.16.attention.self.query.weight\", \"model.roberta.encoder.layer.16.attention.self.query.bias\", \"model.roberta.encoder.layer.16.attention.self.key.weight\", \"model.roberta.encoder.layer.16.attention.self.key.bias\", \"model.roberta.encoder.layer.16.attention.self.value.weight\", \"model.roberta.encoder.layer.16.attention.self.value.bias\", \"model.roberta.encoder.layer.16.attention.output.dense.weight\", \"model.roberta.encoder.layer.16.attention.output.dense.bias\", \"model.roberta.encoder.layer.16.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.16.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.16.intermediate.dense.weight\", \"model.roberta.encoder.layer.16.intermediate.dense.bias\", \"model.roberta.encoder.layer.16.output.dense.weight\", \"model.roberta.encoder.layer.16.output.dense.bias\", \"model.roberta.encoder.layer.16.output.LayerNorm.weight\", \"model.roberta.encoder.layer.16.output.LayerNorm.bias\", \"model.roberta.encoder.layer.17.attention.self.query.weight\", \"model.roberta.encoder.layer.17.attention.self.query.bias\", \"model.roberta.encoder.layer.17.attention.self.key.weight\", \"model.roberta.encoder.layer.17.attention.self.key.bias\", \"model.roberta.encoder.layer.17.attention.self.value.weight\", \"model.roberta.encoder.layer.17.attention.self.value.bias\", \"model.roberta.encoder.layer.17.attention.output.dense.weight\", \"model.roberta.encoder.layer.17.attention.output.dense.bias\", \"model.roberta.encoder.layer.17.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.17.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.17.intermediate.dense.weight\", \"model.roberta.encoder.layer.17.intermediate.dense.bias\", \"model.roberta.encoder.layer.17.output.dense.weight\", \"model.roberta.encoder.layer.17.output.dense.bias\", \"model.roberta.encoder.layer.17.output.LayerNorm.weight\", \"model.roberta.encoder.layer.17.output.LayerNorm.bias\", \"model.roberta.encoder.layer.18.attention.self.query.weight\", \"model.roberta.encoder.layer.18.attention.self.query.bias\", \"model.roberta.encoder.layer.18.attention.self.key.weight\", \"model.roberta.encoder.layer.18.attention.self.key.bias\", \"model.roberta.encoder.layer.18.attention.self.value.weight\", \"model.roberta.encoder.layer.18.attention.self.value.bias\", \"model.roberta.encoder.layer.18.attention.output.dense.weight\", \"model.roberta.encoder.layer.18.attention.output.dense.bias\", \"model.roberta.encoder.layer.18.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.18.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.18.intermediate.dense.weight\", \"model.roberta.encoder.layer.18.intermediate.dense.bias\", \"model.roberta.encoder.layer.18.output.dense.weight\", \"model.roberta.encoder.layer.18.output.dense.bias\", \"model.roberta.encoder.layer.18.output.LayerNorm.weight\", \"model.roberta.encoder.layer.18.output.LayerNorm.bias\", \"model.roberta.encoder.layer.19.attention.self.query.weight\", \"model.roberta.encoder.layer.19.attention.self.query.bias\", \"model.roberta.encoder.layer.19.attention.self.key.weight\", \"model.roberta.encoder.layer.19.attention.self.key.bias\", \"model.roberta.encoder.layer.19.attention.self.value.weight\", \"model.roberta.encoder.layer.19.attention.self.value.bias\", \"model.roberta.encoder.layer.19.attention.output.dense.weight\", \"model.roberta.encoder.layer.19.attention.output.dense.bias\", \"model.roberta.encoder.layer.19.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.19.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.19.intermediate.dense.weight\", \"model.roberta.encoder.layer.19.intermediate.dense.bias\", \"model.roberta.encoder.layer.19.output.dense.weight\", \"model.roberta.encoder.layer.19.output.dense.bias\", \"model.roberta.encoder.layer.19.output.LayerNorm.weight\", \"model.roberta.encoder.layer.19.output.LayerNorm.bias\", \"model.roberta.encoder.layer.20.attention.self.query.weight\", \"model.roberta.encoder.layer.20.attention.self.query.bias\", \"model.roberta.encoder.layer.20.attention.self.key.weight\", \"model.roberta.encoder.layer.20.attention.self.key.bias\", \"model.roberta.encoder.layer.20.attention.self.value.weight\", \"model.roberta.encoder.layer.20.attention.self.value.bias\", \"model.roberta.encoder.layer.20.attention.output.dense.weight\", \"model.roberta.encoder.layer.20.attention.output.dense.bias\", \"model.roberta.encoder.layer.20.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.20.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.20.intermediate.dense.weight\", \"model.roberta.encoder.layer.20.intermediate.dense.bias\", \"model.roberta.encoder.layer.20.output.dense.weight\", \"model.roberta.encoder.layer.20.output.dense.bias\", \"model.roberta.encoder.layer.20.output.LayerNorm.weight\", \"model.roberta.encoder.layer.20.output.LayerNorm.bias\", \"model.roberta.encoder.layer.21.attention.self.query.weight\", \"model.roberta.encoder.layer.21.attention.self.query.bias\", \"model.roberta.encoder.layer.21.attention.self.key.weight\", \"model.roberta.encoder.layer.21.attention.self.key.bias\", \"model.roberta.encoder.layer.21.attention.self.value.weight\", \"model.roberta.encoder.layer.21.attention.self.value.bias\", \"model.roberta.encoder.layer.21.attention.output.dense.weight\", \"model.roberta.encoder.layer.21.attention.output.dense.bias\", \"model.roberta.encoder.layer.21.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.21.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.21.intermediate.dense.weight\", \"model.roberta.encoder.layer.21.intermediate.dense.bias\", \"model.roberta.encoder.layer.21.output.dense.weight\", \"model.roberta.encoder.layer.21.output.dense.bias\", \"model.roberta.encoder.layer.21.output.LayerNorm.weight\", \"model.roberta.encoder.layer.21.output.LayerNorm.bias\", \"model.roberta.encoder.layer.22.attention.self.query.weight\", \"model.roberta.encoder.layer.22.attention.self.query.bias\", \"model.roberta.encoder.layer.22.attention.self.key.weight\", \"model.roberta.encoder.layer.22.attention.self.key.bias\", \"model.roberta.encoder.layer.22.attention.self.value.weight\", \"model.roberta.encoder.layer.22.attention.self.value.bias\", \"model.roberta.encoder.layer.22.attention.output.dense.weight\", \"model.roberta.encoder.layer.22.attention.output.dense.bias\", \"model.roberta.encoder.layer.22.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.22.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.22.intermediate.dense.weight\", \"model.roberta.encoder.layer.22.intermediate.dense.bias\", \"model.roberta.encoder.layer.22.output.dense.weight\", \"model.roberta.encoder.layer.22.output.dense.bias\", \"model.roberta.encoder.layer.22.output.LayerNorm.weight\", \"model.roberta.encoder.layer.22.output.LayerNorm.bias\", \"model.roberta.encoder.layer.23.attention.self.query.weight\", \"model.roberta.encoder.layer.23.attention.self.query.bias\", \"model.roberta.encoder.layer.23.attention.self.key.weight\", \"model.roberta.encoder.layer.23.attention.self.key.bias\", \"model.roberta.encoder.layer.23.attention.self.value.weight\", \"model.roberta.encoder.layer.23.attention.self.value.bias\", \"model.roberta.encoder.layer.23.attention.output.dense.weight\", \"model.roberta.encoder.layer.23.attention.output.dense.bias\", \"model.roberta.encoder.layer.23.attention.output.LayerNorm.weight\", \"model.roberta.encoder.layer.23.attention.output.LayerNorm.bias\", \"model.roberta.encoder.layer.23.intermediate.dense.weight\", \"model.roberta.encoder.layer.23.intermediate.dense.bias\", \"model.roberta.encoder.layer.23.output.dense.weight\", \"model.roberta.encoder.layer.23.output.dense.bias\", \"model.roberta.encoder.layer.23.output.LayerNorm.weight\", \"model.roberta.encoder.layer.23.output.LayerNorm.bias\". \n\tsize mismatch for model.roberta.embeddings.word_embeddings.weight: copying a param with shape torch.Size([32005, 768]) from checkpoint, the shape in current model is torch.Size([32005, 1024]).\n\tsize mismatch for model.roberta.embeddings.position_embeddings.weight: copying a param with shape torch.Size([514, 768]) from checkpoint, the shape in current model is torch.Size([514, 1024]).\n\tsize mismatch for model.roberta.embeddings.token_type_embeddings.weight: copying a param with shape torch.Size([1, 768]) from checkpoint, the shape in current model is torch.Size([1, 1024]).\n\tsize mismatch for model.roberta.embeddings.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.embeddings.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.0.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.0.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.0.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.1.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.1.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.1.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.2.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.2.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.2.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.3.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.3.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.3.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.4.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.4.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.4.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.5.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.5.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.5.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.6.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.6.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.6.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.7.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.7.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.7.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.8.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.8.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.8.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.9.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.9.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.9.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.10.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.10.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.10.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.self.query.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.self.query.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.self.key.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.self.key.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.self.value.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.self.value.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.output.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.attention.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.intermediate.dense.weight: copying a param with shape torch.Size([3072, 768]) from checkpoint, the shape in current model is torch.Size([4096, 1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.intermediate.dense.bias: copying a param with shape torch.Size([3072]) from checkpoint, the shape in current model is torch.Size([4096]).\n\tsize mismatch for model.roberta.encoder.layer.11.output.dense.weight: copying a param with shape torch.Size([768, 3072]) from checkpoint, the shape in current model is torch.Size([1024, 4096]).\n\tsize mismatch for model.roberta.encoder.layer.11.output.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.output.LayerNorm.weight: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.roberta.encoder.layer.11.output.LayerNorm.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.classifier.dense.weight: copying a param with shape torch.Size([768, 768]) from checkpoint, the shape in current model is torch.Size([1024, 1024]).\n\tsize mismatch for model.classifier.dense.bias: copying a param with shape torch.Size([768]) from checkpoint, the shape in current model is torch.Size([1024]).\n\tsize mismatch for model.classifier.out_proj.weight: copying a param with shape torch.Size([2, 768]) from checkpoint, the shape in current model is torch.Size([2, 1024])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Load the best model checkpoint\n",
    "best_model_path = model_checkpoint.best_model_path\n",
    "best_model = LightningModel.load_from_checkpoint(best_model_path, model_name='camembert/camembert-large', num_labels=2, lr=3e-5, weight_decay=0.)\n",
    "\n",
    "# Predict on the test dataset\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "best_model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():  # Disable gradient calculations for inference\n",
    "    for batch in test_dataloader:\n",
    "        preds = best_model.predict_step(batch, batch_idx=0).cpu().numpy()\n",
    "        labels = batch[\"labels\"].cpu().numpy()\n",
    "        \n",
    "        predictions.extend(preds)\n",
    "        true_labels.extend(labels)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions, average='macro')\n",
    "recall = recall_score(true_labels, predictions, average='macro')\n",
    "f1 = f1_score(true_labels, predictions, average='macro')\n",
    "conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')\n",
    "print('Confusion Matrix:')\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e47088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   5,   94,  173,  ...,    1,    1,    1],\n",
       "         [   5,  364,   29,  ...,    1,    1,    1],\n",
       "         [   5,  120,  287,  ...,    1,    1,    1],\n",
       "         ...,\n",
       "         [   5, 5485,   38,  ...,    1,    1,    1],\n",
       "         [   5,  941,    7,  ...,    1,    1,    1],\n",
       "         [   5,    7, 2431,  ...,    1,    1,    1]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'labels': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'str_labels': ['absent',\n",
       "  'absent',\n",
       "  'absent',\n",
       "  'absent',\n",
       "  'absent',\n",
       "  'absent',\n",
       "  'absent',\n",
       "  'absent',\n",
       "  'absent',\n",
       "  'absent',\n",
       "  'absent',\n",
       "  'absent',\n",
       "  'absent',\n",
       "  'absent',\n",
       "  'absent',\n",
       "  'absent'],\n",
       " 'sentences': [\"En cas d'hystéroscopie AVANT LA MÉNOPAUSE, merci de réaliser une PRISE DE SANG dans un laboratoire d'analyses médicales, quelques jours avant l'intervention, et apporter vos résultats : - bétahCG quantitatifs 2) La veille au soir et le matin de l\\x92intervention, prendre une douche avec : - BETADINE SCRUB 125ml 1 flacon - En cas d allergie à la BETADINE, utiliser : HIBISCRUB 125ml 1 flacon 3) Prise en charge de la douleur post opératoire : - IXPRIM : 1 à 2 comprimé(s) matin, midi, soir et nuit\",\n",
       "  'prendre une douche avec : - BETADINE SCRUB 125ml 1 flacon - En cas d allergie à la BETADINE, utiliser : HIBISCRUB 125ml 1 flacon 3) Prise en charge de la douleur post opératoire : - IXPRIM : 1 à 2 comprimé(s) matin, midi, soir et nuit pendant 2 jours - KETOPROFENE : 50 mg matin, midi et soir pendant les repas, pendant 3 à 5 jours - Si antécédents gastriques associer au KETOPROFENE : OMEPRAZOLE : 10 mg le matin pendant 3 à 5 jours - Au 3ème jour post-opératoire, arrêter IXPRIM et',\n",
       "  \"2 jours - KETOPROFENE : 50 mg matin, midi et soir pendant les repas, pendant 3 à 5 jours - Si antécédents gastriques associer au KETOPROFENE : OMEPRAZOLE : 10 mg le matin pendant 3 à 5 jours - Au 3ème jour post-opératoire, arrêter IXPRIM et prendre à la place, si besoin : PARACETAMOL : 1 gramme matin midi soir et nuit pendant 5 jours 5) CHAUSSETTES DE CONTENTION force 2 : 2 paires, à porter la journée pendant 1 mois Ce document tient lieu d ordonnance pour le laboratoire, l'infirmière\",\n",
       "  \"50 mg matin, midi et soir pendant les repas, pendant 3 à 5 jours - Si antécédents gastriques associer au KETOPROFENE : OMEPRAZOLE : 10 mg le matin pendant 3 à 5 jours - Au 3ème jour post-opératoire, arrêter IXPRIM et prendre à la place, si besoin : PARACETAMOL : 1 gramme matin midi soir et nuit pendant 5 jours 5) CHAUSSETTES DE CONTENTION force 2 : 2 paires, à porter la journée pendant 1 mois Ce document tient lieu d ordonnance pour le laboratoire, l'infirmière et la\",\n",
       "  \"VB sans complications le 15/08/2020 sortie de la maternité le 18/08 pas d?allaitement maternelle G5P3 : - AVB avec ventouse 2009 - AVB en 2017 : Rétention placentaire Depuis le 27/08 : DOuleurs pelviennes d'intensité progressive : douleurs continuelle comme une pesanteur pelvienne sans position antalgique EVA 7/10, pics douloureux à type de contractions réguliers latéralisé à gauche EVA 10 + -> prise de Spasfon + doliprane à domicile sans soulagement quelques douleurs lombaires\",\n",
       "  \"continuelle comme une pesanteur pelvienne sans position antalgique EVA 7/10, pics douloureux à type de contractions réguliers latéralisé à gauche EVA 10 + -> prise de Spasfon + doliprane à domicile sans soulagement quelques douleurs lombaires MTR depuis l'accouchement sans majoration, rouges parfois caillots pas de leucorrhées pas de fièvre ressentie à domicile pas de trouble du transit, pas de nausées/vomissements Leuco 9.7 Hb 133 iono N CRP 36 A l'examen : douleurs pelviennes non\",\n",
       "  \"l'accouchement sans majoration, rouges parfois caillots pas de leucorrhées pas de fièvre ressentie à domicile pas de trouble du transit, pas de nausées/vomissements Leuco 9.7 Hb 133 iono N CRP 36 A l'examen : douleurs pelviennes non latéralisées : EVA 7/10 apyrétique 36.8° TA 125/85 FC 89 EEV: hématométrie de 7.5 mm sans rétention, annexes ras, minime ép du douglas, passage sonde sensible TV: douloureux à la mobilisation annexielle Spécu: col sain, pas de pertes patho, PV fait Au total\",\n",
       "  \"de fièvre ressentie à domicile pas de trouble du transit, pas de nausées/vomissements Leuco 9.7 Hb 133 iono N CRP 36 A l'examen : douleurs pelviennes non latéralisées : EVA 7/10 apyrétique 36.8° TA 125/85 FC 89 EEV: hématométrie de 7.5 mm sans rétention, annexes ras, minime ép du douglas, passage sonde sensible TV: douloureux à la mobilisation annexielle Spécu: col sain, pas de pertes patho, PV fait Au total, probable endométrite du pp CAT: Augmentin 10j Antalgiques Contrôle clinique dans\",\n",
       "  'de 32 ans à 16 + 3 SA consulte pour contractions ATCD: - IU: 1/an en ce moment, avant plus fréquentes, traitées par Monuril au coup par coup - CNA en 2010 avec LEC - CNA pendant la 2ème grossesse avec hospitalisation en grossesse patho au 7ème mois 1 semaine non surinfectée, - G5P2 2 FCS (1 précoce une plus tardive) 2 AVB 2009 (RAS) et 2017: alitement + repos non strict à partir du 5ème mois pour MAP: contractions douloureuses = COL à D2, AVB à terme à 40 SA Serologies: - Toxo',\n",
       "  \"mois 1 semaine non surinfectée, - G5P2 2 FCS (1 précoce une plus tardive) 2 AVB 2009 (RAS) et 2017: alitement + repos non strict à partir du 5ème mois pour MAP: contractions douloureuses = COL à D2, AVB à terme à 40 SA Serologies: - Toxo -, Rubéole immunisée, HBV -, VIH -, Syphilis - Echo T1 le 27/01, DG: 14/11: Pas d'anomalie morpho, CN N (0.9 mm), AC +, QLAN HDM: 16 SA + 3j, grossesse inopinée Suivie par SF X rousse A vu son MT pour le suivi de grossesse début février: RAS: Toxo pas de\",\n",
       "  \"immunisée, HBV -, VIH -, Syphilis - Echo T1 le 27/01, DG: 14/11: Pas d'anomalie morpho, CN N (0.9 mm), AC +, QLAN HDM: 16 SA + 3j, grossesse inopinée Suivie par SF X rousse A vu son MT pour le suivi de grossesse début février: RAS: Toxo pas de séroconversion, BU glucose prot normale. Perte de poids par rapport à avant la grossesse à ce moment (70 vs 72 kg) Depuis quelques jours, douleurs hypogastriques comme des contractions mais sans l'impression de ventre dur qu'elle pouvait avoir lors de\",\n",
       "  \"BU glucose prot normale. Perte de poids par rapport à avant la grossesse à ce moment (70 vs 72 kg) Depuis quelques jours, douleurs hypogastriques comme des contractions mais sans l'impression de ventre dur qu'elle pouvait avoir lors de contractions. Ce matin toutes les 5 minutes environ Vomissement + nausées importantes depuis 3 semaines - 1 mois avec difficultés à la prise alimentaire: perte de poids selon la patiente, n'arrive pas à s'alimenter correctement. Pas de MTR Pas de\",\n",
       "  \"Ce matin toutes les 5 minutes environ Vomissement + nausées importantes depuis 3 semaines - 1 mois avec difficultés à la prise alimentaire: perte de poids selon la patiente, n'arrive pas à s'alimenter correctement. Pas de MTR Pas de perte de liquide aminiotique Impression de pollakiurie, pas de brûlures mictionnelles Pas de douleurs abdo hormis les contractions Pas de fièvre Clinique: apyretique (37°C) abdomen souple sensible de manière general difusement pas de defense TV\",\n",
       "  \"liquide aminiotique Impression de pollakiurie, pas de brûlures mictionnelles Pas de douleurs abdo hormis les contractions Pas de fièvre Clinique: apyretique (37°C) abdomen souple sensible de manière general difusement pas de defense TV posterieur long dehiscent OE EEV: 31mm entonnoir col origenllement à 67mm QLAN MAF+ Pas de douleurs à l'ébranlement des fosses lombaires BU négative CCl: douleurs abdo à 16 SA + 3 avec possibles contractions pas de mofication cervicale franche Repos AT\",\n",
       "  \"Pas de douleurs abdo hormis les contractions Pas de fièvre Clinique: apyretique (37°C) abdomen souple sensible de manière general difusement pas de defense TV posterieur long dehiscent OE EEV: 31mm entonnoir col origenllement à 67mm QLAN MAF+ Pas de douleurs à l'ébranlement des fosses lombaires BU négative CCl: douleurs abdo à 16 SA + 3 avec possibles contractions pas de mofication cervicale franche Repos AT Antalgiques puis reevaluation avec sa SF debut mars, Consultations urgences\",\n",
       "  'de 32 ans à 16 + 3 SA consulte pour contractions ATCD: - IU: 1/an en ce moment, avant plus fréquentes, traitées par Monuril au coup par coup - CNA en 2010 avec LEC - CNA pendant la 2ème grossesse avec hospitalisation en grossesse patho au 7ème mois 1 semaine non surinfectée, - G5P2 2 FCS (1 précoce une plus tardive) 2 AVB 2009 (RAS) et 2017: alitement + repos non strict à partir du 5ème mois pour MAP: contractions douloureuses = COL à D2, AVB à terme à 40 SA Serologies: - Toxo']}"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(val_dataloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc-autonumbering": true,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
